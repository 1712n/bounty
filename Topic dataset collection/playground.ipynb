{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nsd/.local/share/virtualenvs/Inca_task-0ZlLqLYO/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from gnews import GNews\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import stweet as st\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICS = {\"withdrawal\": {\"frozen wallets\"},\n",
    "          \"fraud\": {\"front running\", \"wash trading\", \"exit scam\"}, \n",
    "          \"hacker attacks\": {\"DDoS\", \"breach\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crypto custodians list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "\n",
    "url = 'https://www.cointelligence.com/exchanges_list/'\n",
    "\n",
    "header = {\n",
    "  \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36\",\n",
    "  \"X-Requested-With\": \"XMLHttpRequest\"\n",
    "}\n",
    "\n",
    "r = requests.get(url, headers=header)\n",
    "\n",
    "dfs = pd.read_html(r.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "exchanges_list = dfs[0][\"Exchanges\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "googlenews = GNews()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_dates_for_custodies(custodies, news_api):\n",
    "    dates_by_custodies = {}\n",
    "    articles_by_custodies = {}\n",
    "    for i, crypto in tqdm(enumerate(custodies), leave=True, position=0):\n",
    "        dates = []\n",
    "        articles = []\n",
    "        crypto_news = news_api.get_news(crypto)\n",
    "        for news in crypto_news:\n",
    "            articles.append(news[\"title\"])\n",
    "            date_string = news[\"published date\"][:-4].rstrip()\n",
    "            datetime_object = datetime.strptime(date_string, '%a, %d %b %Y %H:%M:%S').strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            dates.append(datetime_object)\n",
    "        dates_by_custodies[crypto] = dates\n",
    "        articles_by_custodies[crypto] = articles\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            with open(os.path.join(DATA_PATH, \"dates_by_custodies\"), 'w') as fp:\n",
    "                json.dump(dates_by_custodies, fp)\n",
    "    return dates_by_custodies, articles_by_custodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "341it [2:16:16, 23.98s/it]\n"
     ]
    }
   ],
   "source": [
    "dates_by_custodies, articles_by_custodies = news_dates_for_custodies(exchanges_list, googlenews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_PATH, \"dates_by_custodies.json\"), 'w') as fp:\n",
    "    json.dump(dates_by_custodies, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_dates(dates_by_custodies):\n",
    "    best_dates_by_crypto = {}\n",
    "    for crypto, dates in tqdm(dates_by_custodies.items()):\n",
    "        if len(dates) == 0:\n",
    "            continue\n",
    "        most_frequent_date = max(set(dates), key = dates.count)\n",
    "        most_frequent_date = datetime.strptime(most_frequent_date, \"%Y-%m-%d %H:%M:%S\").date()\n",
    "        interval = (most_frequent_date - timedelta(days=10), most_frequent_date + timedelta(days=10))\n",
    "        best_dates_by_crypto[crypto] = interval\n",
    "    return best_dates_by_crypto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 341/341 [00:00<00:00, 19705.40it/s]\n"
     ]
    }
   ],
   "source": [
    "best_dates_by_crypto = best_dates(dates_by_custodies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "331"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(best_dates_by_crypto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synonymous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/nsd/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syn_find(topics):\n",
    "    for topic in topics.keys():\n",
    "        synonyms = set()\n",
    "        for syn in wordnet.synsets(topic):\n",
    "            for lm in syn.lemmas():\n",
    "                    topics[topic].add(lm.name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets extracting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('creds.json') as f:\n",
    "   creds = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = creds[\"api_key\"]\n",
    "consumer_secret = creds[\"api_key_secret\"]\n",
    "access_token = creds[\"access_token\"]\n",
    "access_token_secret = creds[\"access_token_secret\"]\n",
    "bearer_token = creds[\"bearer_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_search(fix_words, additional, store_path):\n",
    "    search_tweets_task = st.SearchTweetsTask(all_words=fix_words, any_word=additional)\n",
    "    output_jl_tweets = st.JsonLineFileRawOutput(os.path.join(store_path, f'{fix_words}_raw_tweets.jl'))\n",
    "    output_jl_users = st.JsonLineFileRawOutput('output_raw_search_users.jl')\n",
    "    output_print = st.PrintRawOutput()\n",
    "    st.TweetSearchRunner(search_tweets_task=search_tweets_task,\n",
    "                         tweet_raw_data_outputs=[output_jl_tweets],\n",
    "                         user_raw_data_outputs=[output_jl_users]).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_search(\"Binance\", \"\", \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tweets(custodies, themes):\n",
    "    for crypto in tqdm(custodies):\n",
    "        crypto_path = os.path.join(DATA_PATH, crypto)\n",
    "        if not os.path.exists(crypto_path):\n",
    "            os.makedirs(crypto_path)\n",
    "        for theme, synon in themes.items():\n",
    "            main_search = crypto + \" \" + theme\n",
    "            possible_words = \" \".join(synon)\n",
    "            try_search(main_search, possible_words, crypto_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 341/341 [47:02<00:00,  8.28s/it] \n"
     ]
    }
   ],
   "source": [
    "make_tweets(list(dates_by_custodies.keys()), TOPICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"
     ]
    }
   ],
   "source": [
    "specific_model = pipeline(model=\"finiteautomata/bertweet-base-sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(\"./Binance_raw_tweets.jl\") as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stop playing guysðŸ˜‚ \\n\\n#Binance #BTC #v501s #asian https://t.co/TmwOi9OqZx'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[50]['raw_value']['full_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subfolders = [f.path for f in os.scandir(DATA_PATH) if f.is_dir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'NEG', 'score': 0.9488999843597412}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specific_model(\"Fuck\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'withdrawal': {'frozen wallets'},\n",
       " 'fraud': {'exit scam', 'front running', 'wash trading'},\n",
       " 'hacker attacks': {'DDoS', 'breach'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOPICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_cleaning(model, path, threshold):\n",
    "    all_negative_tweets = []\n",
    "    withdrawal = []\n",
    "    fraud = []\n",
    "    hackers_attack = []\n",
    "    subfolders = [f.path for f in os.scandir(path) if f.is_dir()]\n",
    "    tweets_logs = open('all_neg_tweets.txt', 'w')\n",
    "    for folder in tqdm(subfolders):\n",
    "        all_files = os.listdir(folder)\n",
    "        for file in all_files:\n",
    "            data = []\n",
    "            with open(os.path.join(folder, file)) as f:\n",
    "                for line in f:\n",
    "                    data.append(json.loads(line))\n",
    "            if len(data) == 0:\n",
    "                continue\n",
    "            \n",
    "            for tweet in data:\n",
    "                text = tweet['raw_value']['full_text']\n",
    "                try:\n",
    "                    res = model(text)[0]\n",
    "                except:\n",
    "                    pass\n",
    "                if res['label'] == 'NEG':\n",
    "                    if res['score'] > threshold:\n",
    "                        all_negative_tweets.append(text)\n",
    "                        tweets_logs.write(text + '\\n')\n",
    "                        if 'withdrawal' in file:\n",
    "                            withdrawal.append(text)\n",
    "                        elif 'fraud' in file:\n",
    "                            fraud.append(text)\n",
    "                        else:\n",
    "                            hackers_attack.append(text)\n",
    "    tweets_logs.close()\n",
    "    return (all_negative_tweets, withdrawal, fraud, hackers_attack)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 341/341 [10:50<00:00,  1.91s/it]\n"
     ]
    }
   ],
   "source": [
    "results = sent_cleaning(specific_model, DATA_PATH, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5391"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_negative_tweets_60 = results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_negative_tweets_80 = results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(os.path.join(DATA_PATH, 'results'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_PATH, 'results', 'negative_tweets.txt'), 'w') as f:\n",
    "    for tweet in all_negative_tweets_80:\n",
    "        f.write(f\"{tweet}\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "230e3cedceaee9165ef30ee4e1a7ecc3827a884c7ef04ce50d672d9520c69aa4"
  },
  "kernelspec": {
   "display_name": "Python 3.11.2 ('Inca_task-0ZlLqLYO')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
